# -*- coding: utf-8 -*-
"""ReverseEngineeringAssistantV2.0_RAG_PDF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18qjhyE5yfnfGaSdYTUyIlu7GmHCtgiFH

#Reverse Engineering Assistant V2.0
"""

!pip install langchain langchain_community langchain_chroma langchain-openai langchainhub unstructured[pdf]

import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o")

#Using the UnstructuredPDFLoader for loading the document
from langchain_community.document_loaders import UnstructuredPDFLoader

#Create the loader
loader = UnstructuredPDFLoader("/content/drive/MyDrive/Book PDFs/BlackHatGo.pdf")

#Get the loaded data
data = loader.load()

#Importing the libraries
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain import hub
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.chains import TransformChain
from langchain.chat_models import ChatOpenAI
import sys

#Splitting the data and creating a vectorestore
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(data)
vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())

vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 6})
prompt = hub.pull("byteberzerker/reverse_helper")

#Test Retrieved docs
#retrieved_docs = retriever.invoke("How do I analyze a binary?")
#print(retrieved_docs)
#x = len(retrieved_docs)
#print(x)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Get user input for question which is then passed to rag_chain.invoke function
user_question = input("Please enter your question: ")
result = rag_chain.invoke(user_question)



# Function to print the result nicely
# Function to print the result nicely
def print_nice_result(result):
    print("\n=== RAG Chain Result ===\n")
    print(result)
    print("=" * 40)

# Print the nicely formatted result
print_nice_result(result)

# cleanup
vectorstore.delete_collection()